# backend/embeddings.py
"""
è½»é‡çº§æ–‡æœ¬å‘é‡åŒ–æ¨¡å—ï¼ˆåŸºäº HashingVectorizerï¼‰
âœ… æ—  OOM é£é™© | âœ… å†…å­˜æ’å®š | âœ… æ— éœ€ fit | âœ… æ”¯æŒæµå¼å¤„ç†
"""

from sklearn.feature_extraction.text import HashingVectorizer
from typing import List
import gc

# ğŸ”¥ å…¨å±€å•ä¾‹ï¼šHashingVectorizerï¼ˆå†…å­˜å›ºå®šï¼Œæ— éœ€è®­ç»ƒï¼‰
_vectorizer = HashingVectorizer(
    n_features=1024,           # å›ºå®šç»´åº¦ï¼ˆ1KB~10MB çº§æ–‡æ¡£è¶³å¤Ÿï¼‰
    analyzer="word",           # æŒ‰è¯åˆ†å‰²
    ngram_range=(1, 1),        # ä»… unigramï¼ˆæ›´å®‰å…¨ï¼Œbigram æ˜“è†¨èƒ€ï¼‰
    stop_words="english",      # è¿‡æ»¤å¸¸è§è¯
    alternate_sign=False,      # é¿å…è´Ÿæ•°ç‰¹å¾
    norm="l2"                  # å½’ä¸€åŒ–ï¼Œä¾¿äº cosine ç›¸ä¼¼åº¦è®¡ç®—
)

# ğŸ“„ å­˜å‚¨åŸå§‹æ–‡æœ¬ï¼ˆç”¨äºæ£€ç´¢åè¿”å›ï¼‰
_texts = []

def build_embeddings(texts: List[str]):
    """
    å“ˆå¸Œå‘é‡åŒ–ï¼šæ— éœ€æ‹Ÿåˆï¼Œç›´æ¥å­˜å‚¨æ–‡æœ¬
    Args:
        texts: æ–‡æœ¬å—åˆ—è¡¨
    Returns:
        Noneï¼ˆå‘é‡åœ¨ transform æ—¶å®æ—¶ç”Ÿæˆï¼‰
    """
    global _texts
    print(f"[INFO] ğŸ”¹ Received {len(texts)} chunks for indexing (HashingVectorizer)", flush=True)
    
    # æ¸…ç†æ—§æ•°æ®ï¼ˆå¯é€‰ï¼‰
    if _texts:
        print("[INFO] ğŸ§¹ Clearing previous text store...", flush=True)
    
    _texts = texts.copy()  # ä¿å­˜æ–‡æœ¬ç”¨äºåç»­æ£€ç´¢
    print(f"[INFO] âœ… Text store updated with {len(_texts)} chunks.", flush=True)
    
    # å¼ºåˆ¶åƒåœ¾å›æ”¶
    gc.collect()
    return None  # ä¸è¿”å›çŸ©é˜µ


def get_embeddings(texts: List[str]):
    """
    ä¸ºæŸ¥è¯¢æˆ–æ–°æ–‡æœ¬ç”Ÿæˆå“ˆå¸Œå‘é‡
    Returns:
        scipy.sparse matrix (n_samples, 1024)
    """
    if not _texts:
        raise RuntimeError("No documents indexed. Please upload a file first.")
    return _vectorizer.transform(texts)


def get_texts() -> List[str]:
    """è·å–æ‰€æœ‰å·²ç´¢å¼•çš„æ–‡æœ¬å—"""
    return _texts


def is_fitted() -> bool:
    """HashingVectorizer æ— éœ€æ‹Ÿåˆï¼Œå§‹ç»ˆå¯ç”¨"""
    return len(_texts) > 0


def clear_index():
    """æ¸…ç©ºç´¢å¼•ï¼ˆå¯é€‰ï¼‰"""
    global _texts
    _texts = []
    gc.collect()