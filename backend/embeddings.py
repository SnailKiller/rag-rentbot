# backend/embeddings.py
from sklearn.feature_extraction.text import HashingVectorizer
from typing import List
import gc

# ğŸ”¥ å…¨å±€å•ä¾‹ï¼šHashingVectorizerï¼ˆå†…å­˜å›ºå®šï¼Œæ— éœ€è®­ç»ƒï¼‰
_vectorizer = HashingVectorizer(
    n_features=1024,           # å›ºå®šç»´åº¦ï¼ˆ1KB~10MB çº§æ–‡æ¡£è¶³å¤Ÿï¼‰
    analyzer="word",           # æŒ‰è¯åˆ†å‰²
    ngram_range=(1, 1),        # ä»… unigramï¼ˆæ›´å®‰å…¨ï¼Œbigram æ˜“è†¨èƒ€ï¼‰
    stop_words="english",      # è¿‡æ»¤å¸¸è§è¯
    alternate_sign=False,      # é¿å…è´Ÿæ•°ç‰¹å¾
    norm="l2"                  # å½’ä¸€åŒ–ï¼Œä¾¿äº cosine ç›¸ä¼¼åº¦è®¡ç®—
)

# ğŸ“„ å­˜å‚¨åŸå§‹æ–‡æœ¬ï¼ˆç”¨äºæ£€ç´¢åè¿”å›ï¼‰
_texts = []

def build_embeddings(texts: List[str]):
    global _texts
    print(f"[INFO] ğŸ”¹ Received {len(texts)} chunks for indexing (HashingVectorizer)", flush=True)

    if _texts:
        print("[INFO] ğŸ§¹ Clearing previous text store...", flush=True)
    
    _texts = texts.copy()  # ä¿å­˜æ–‡æœ¬ç”¨äºåç»­æ£€ç´¢
    print(f"[INFO] âœ… Text store updated with {len(_texts)} chunks.", flush=True)
    
    # å¼ºåˆ¶åƒåœ¾å›æ”¶
    gc.collect()
    return None  # ä¸è¿”å›çŸ©é˜µ


def get_embeddings(texts: List[str]):
    if not _texts:
        raise RuntimeError("No documents indexed. Please upload a file first.")
    return _vectorizer.transform(texts)


def get_texts() -> List[str]:
    """è·å–æ‰€æœ‰å·²ç´¢å¼•çš„æ–‡æœ¬å—"""
    return _texts


def is_fitted() -> bool:
    """HashingVectorizer æ— éœ€æ‹Ÿåˆï¼Œå§‹ç»ˆå¯ç”¨"""
    return len(_texts) > 0


def clear_index():
    """æ¸…ç©ºç´¢å¼•ï¼ˆå¯é€‰ï¼‰"""
    global _texts
    _texts = []
    gc.collect()